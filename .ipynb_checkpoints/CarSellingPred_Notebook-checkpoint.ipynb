{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Sale Prediction Analysis According to Segments\n",
    "\n",
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 532 entries, 0 to 531\n",
      "Data columns (total 17 columns):\n",
      "MARKA           64 non-null object\n",
      "Unnamed: 1      162 non-null object\n",
      "MODEL           376 non-null object\n",
      "SEGMENT         321 non-null object\n",
      "200401          524 non-null object\n",
      "200402          524 non-null object\n",
      "200403          524 non-null object\n",
      "200404          524 non-null object\n",
      "200405          524 non-null object\n",
      "200406          508 non-null object\n",
      "200407          508 non-null object\n",
      "200408          508 non-null object\n",
      "200409          508 non-null object\n",
      "200410          508 non-null object\n",
      "200411          508 non-null object\n",
      "200412          508 non-null object\n",
      "Toplam/Total    508 non-null object\n",
      "dtypes: object(17)\n",
      "memory usage: 70.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MARKA</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>SEGMENT</th>\n",
       "      <th>200401</th>\n",
       "      <th>200402</th>\n",
       "      <th>200403</th>\n",
       "      <th>200404</th>\n",
       "      <th>200405</th>\n",
       "      <th>200406</th>\n",
       "      <th>200407</th>\n",
       "      <th>200408</th>\n",
       "      <th>200409</th>\n",
       "      <th>200410</th>\n",
       "      <th>200411</th>\n",
       "      <th>200412</th>\n",
       "      <th>Toplam/Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>64</td>\n",
       "      <td>162</td>\n",
       "      <td>376</td>\n",
       "      <td>321</td>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>524</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "      <td>291</td>\n",
       "      <td>45</td>\n",
       "      <td>194</td>\n",
       "      <td>186</td>\n",
       "      <td>207</td>\n",
       "      <td>219</td>\n",
       "      <td>226</td>\n",
       "      <td>208</td>\n",
       "      <td>194</td>\n",
       "      <td>199</td>\n",
       "      <td>212</td>\n",
       "      <td>207</td>\n",
       "      <td>205</td>\n",
       "      <td>221</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A (Mini)</td>\n",
       "      <td>Toplam/Total</td>\n",
       "      <td>ASTRA</td>\n",
       "      <td>B2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>193</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>155</td>\n",
       "      <td>149</td>\n",
       "      <td>154</td>\n",
       "      <td>155</td>\n",
       "      <td>150</td>\n",
       "      <td>149</td>\n",
       "      <td>160</td>\n",
       "      <td>148</td>\n",
       "      <td>137</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           MARKA    Unnamed: 1  MODEL SEGMENT  200401  200402  200403  200404  \\\n",
       "count         64           162    376     321     524     524     524     524   \n",
       "unique        58            17    291      45     194     186     207     219   \n",
       "top     A (Mini)  Toplam/Total  ASTRA      B2       0       0       0       0   \n",
       "freq           2            50      5      31     193     186     186     155   \n",
       "\n",
       "        200405  200406  200407  200408  200409  200410  200411  200412  \\\n",
       "count      524     508     508     508     508     508     508     508   \n",
       "unique     226     208     194     199     212     207     205     221   \n",
       "top          0       0       0       0       0       0       0       0   \n",
       "freq       149     154     155     150     149     160     148     137   \n",
       "\n",
       "        Toplam/Total  \n",
       "count            508  \n",
       "unique           321  \n",
       "top                0  \n",
       "freq              78  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_2004 = pd.read_excel(\"MODELDOKUMUARALIK2004.xls\",skiprows=1,index_col=None)\n",
    "df_2005 = pd.read_excel(\"MODELDOKUMUARALIK2005.xls\",skiprows=1,index_col=None)\n",
    "df_2006 = pd.read_excel(\"Model Dökümü, Aralık 2006.xls\",skiprows=1,index_col=None)\n",
    "df_2007 = pd.read_excel(\"MODELDOKUMARALIK2007.xls\",skiprows=1,index_col=None)\n",
    "df_2008 = pd.read_excel(\"MODELDOKUMUARALIK2008.xls\",skiprows=1,index_col=None)\n",
    "df_2009 = pd.read_excel(\"MODELDOKUMUARALIK2009.xls\",skiprows=1,index_col=None)\n",
    "df_2010 = pd.read_excel(\"MODELDOKUMUARALIK2010.xls\",skiprows=1,index_col=None)\n",
    "df_2011 = pd.read_excel(\"MODELDOKUMUARALIK2011.xls\",skiprows=1,index_col=None)\n",
    "df_2012 = pd.read_excel(\"MODELDOKUMARALIK2012.xls\",skiprows=1,index_col=None)\n",
    "df_2013 = pd.read_excel(\"MODELDOKUMUARALIK2013.xls\",skiprows=1,index_col=None)\n",
    "df_2014 = pd.read_excel(\"MODELDOKUMUARALIK2014.xls\",skiprows=1,index_col=None)\n",
    "df_2015 = pd.read_excel(\"MODELDOKUMARALIK2015.xls\",skiprows=2,index_col=None)\n",
    "df_2016 = pd.read_excel(\"Model Dokumu Aralık'2016.xls\",skiprows=2,index_col=None)\n",
    "df_2017 = pd.read_excel(\"Model Dokumu Aralık 2017.xlsx\",skiprows=2,index_col=None)\n",
    "df_2018 = pd.read_excel(\"Model Dokumu Şubat'2018.xls\",skiprows=2,index_col=None)\n",
    "\n",
    "df_2004.info()\n",
    "df_2004.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to retrieve Monthly Sales of Segments and clean unwanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment(df):\n",
    "    df = df.iloc[:-25,3:16]\n",
    "    df['SEGMENT'] = df['SEGMENT'].astype('str')\n",
    "    df = df[df.SEGMENT.str.contains(\"nan\") == False]\n",
    "    df['SEGMENT'] = df['SEGMENT'].apply(lambda x: str(x)[0])\n",
    "    df = df.groupby(['SEGMENT']).sum()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2004 = segment(df_2004)\n",
    "df_2005 = segment(df_2005)\n",
    "df_2006 = segment(df_2006)\n",
    "df_2007 = segment(df_2007)\n",
    "df_2008 = segment(df_2008)\n",
    "df_2009 = segment(df_2009)\n",
    "df_2010 = segment(df_2010)\n",
    "df_2011 = segment(df_2011)\n",
    "df_2012 = segment(df_2012)\n",
    "df_2013 = segment(df_2013)\n",
    "df_2014 = segment(df_2014)\n",
    "df_2015 = segment(df_2015)\n",
    "df_2016 = segment(df_2016)\n",
    "df_2017 = segment(df_2017)\n",
    "df_2018 = segment(df_2018)\n",
    "\n",
    "df_2018 = df_2018.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 170 entries, 200401 to 201802\n",
      "Data columns (total 6 columns):\n",
      "A    170 non-null int64\n",
      "B    170 non-null int64\n",
      "C    170 non-null int64\n",
      "D    170 non-null int64\n",
      "E    170 non-null int64\n",
      "F    170 non-null int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 9.3 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SEGMENT</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>170.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>170.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>252.205882</td>\n",
       "      <td>16107.947059</td>\n",
       "      <td>20767.447059</td>\n",
       "      <td>5412.111765</td>\n",
       "      <td>1146.947059</td>\n",
       "      <td>287.935294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>137.381941</td>\n",
       "      <td>6286.426447</td>\n",
       "      <td>11060.365468</td>\n",
       "      <td>2657.924205</td>\n",
       "      <td>768.974128</td>\n",
       "      <td>144.008429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>4049.000000</td>\n",
       "      <td>4482.000000</td>\n",
       "      <td>1653.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>152.000000</td>\n",
       "      <td>12124.250000</td>\n",
       "      <td>12745.750000</td>\n",
       "      <td>3462.250000</td>\n",
       "      <td>552.750000</td>\n",
       "      <td>185.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>229.000000</td>\n",
       "      <td>15880.000000</td>\n",
       "      <td>18736.000000</td>\n",
       "      <td>4940.000000</td>\n",
       "      <td>944.500000</td>\n",
       "      <td>261.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>19468.250000</td>\n",
       "      <td>25811.250000</td>\n",
       "      <td>6702.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>342.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>948.000000</td>\n",
       "      <td>37670.000000</td>\n",
       "      <td>59812.000000</td>\n",
       "      <td>15515.000000</td>\n",
       "      <td>4025.000000</td>\n",
       "      <td>904.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "SEGMENT           A             B             C             D            E  \\\n",
       "count    170.000000    170.000000    170.000000    170.000000   170.000000   \n",
       "mean     252.205882  16107.947059  20767.447059   5412.111765  1146.947059   \n",
       "std      137.381941   6286.426447  11060.365468   2657.924205   768.974128   \n",
       "min       41.000000   4049.000000   4482.000000   1653.000000   222.000000   \n",
       "25%      152.000000  12124.250000  12745.750000   3462.250000   552.750000   \n",
       "50%      229.000000  15880.000000  18736.000000   4940.000000   944.500000   \n",
       "75%      326.000000  19468.250000  25811.250000   6702.000000  1600.000000   \n",
       "max      948.000000  37670.000000  59812.000000  15515.000000  4025.000000   \n",
       "\n",
       "SEGMENT           F  \n",
       "count    170.000000  \n",
       "mean     287.935294  \n",
       "std      144.008429  \n",
       "min       98.000000  \n",
       "25%      185.750000  \n",
       "50%      261.500000  \n",
       "75%      342.000000  \n",
       "max      904.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2004 = df_2004.transpose()\n",
    "df_2005 = df_2005.transpose()\n",
    "df_2006 = df_2006.transpose()\n",
    "df_2007 = df_2007.transpose()\n",
    "df_2008 = df_2008.transpose()\n",
    "df_2009 = df_2009.transpose()\n",
    "df_2010 = df_2010.transpose()\n",
    "df_2011 = df_2011.transpose()\n",
    "df_2012 = df_2012.transpose()\n",
    "df_2013 = df_2013.transpose()\n",
    "df_2014 = df_2014.transpose()\n",
    "df_2015 = df_2015.transpose()\n",
    "df_2016 = df_2016.transpose()\n",
    "df_2017 = df_2017.transpose()\n",
    "df_2018 = df_2018.transpose()\n",
    "\n",
    "data = pd.concat([df_2004,df_2005,df_2006,df_2007,df_2008,df_2009,df_2010,df_2011,df_2012,df_2013,df_2014,df_2015,df_2016,df_2017,df_2018])\n",
    "del df_2004,df_2005,df_2006,df_2007,df_2008,df_2009,df_2010,df_2011,df_2012,df_2013,df_2014,df_2015,df_2016,df_2017,df_2018\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting Data to access previous informations of that month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "data2 = data2.shift(1)\n",
    "\n",
    "data['Last_Month_A'] = data2['A']\n",
    "data['Last_Month_B'] = data2['B']\n",
    "data['Last_Month_C'] = data2['C']\n",
    "data['Last_Month_D'] = data2['D']\n",
    "data['Last_Month_E'] = data2['E']\n",
    "data['Last_Month_F'] = data2['F']\n",
    "\n",
    "del data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding USD, EUR, BIST 100, Export and Import Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd = pd.read_excel(\"usd_try_monthly.xls\",skiprows = 1, index_col = None)\n",
    "usd = usd.set_index('Tarih')\n",
    "eur = pd.read_excel(\"eur_try_monthly.xls\",skiprows = 1, index_col = None)\n",
    "eur = eur.set_index('Tarih')\n",
    "bist = pd.read_excel(\"bist100monthly.xls\",skiprows = 1, index_col = None)\n",
    "bist = bist.set_index('Tarih')\n",
    "export_import = pd.read_excel(\"Export_Import.xls\")\n",
    "export_import = export_import.set_index('Month')\n",
    "\n",
    "usd = usd.join(eur)\n",
    "del eur\n",
    "usd = usd.join(bist)\n",
    "del bist\n",
    "usd = usd.join(export_import)\n",
    "del export_import\n",
    "\n",
    "last2usd = usd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating previous months information by shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "usd['USD Diff -1'] = usd['USD Diff'].fillna(usd['USD Diff'])\n",
    "usd['EUR Diff -1'] = usd['EUR Diff'].fillna(usd['EUR Diff'])\n",
    "usd['BIST Diff -1'] = usd['BIST Diff'].fillna(usd['BIST Diff'])\n",
    "\n",
    "data['A Sell Diff'] = (100*(data['A'] - data['Last_Month_A'])/data['Last_Month_A'])\n",
    "data['B Sell Diff'] = (100*(data['B'] - data['Last_Month_B'])/data['Last_Month_B'])\n",
    "data['C Sell Diff'] = (100*(data['C'] - data['Last_Month_C'])/data['Last_Month_C'])\n",
    "data['D Sell Diff'] = (100*(data['D'] - data['Last_Month_D'])/data['Last_Month_D'])\n",
    "data['E Sell Diff'] = (100*(data['E'] - data['Last_Month_E'])/data['Last_Month_E'])\n",
    "data['F Sell Diff'] = (100*(data['F'] - data['Last_Month_F'])/data['Last_Month_F'])\n",
    "\n",
    "\n",
    "#for -1 month\n",
    "last2usd = last2usd.shift(1)\n",
    "data2 = data.copy().shift(1)\n",
    "usd['Export Diff -1'] = 100*(usd['Exports'] - last2usd['Exports'])/last2usd['Exports']\n",
    "usd['Import Diff -1'] = 100*(usd['Imports'] - last2usd['Imports'])/last2usd['Imports']\n",
    "\n",
    "data['A Sell Diff -1'] = 100*(data['Last_Month_A'] - data2['Last_Month_A'])/data2['Last_Month_A']\n",
    "data['B Sell Diff -1'] = 100*(data['Last_Month_B'] - data2['Last_Month_B'])/data2['Last_Month_B']\n",
    "data['C Sell Diff -1'] = 100*(data['Last_Month_C'] - data2['Last_Month_C'])/data2['Last_Month_C']\n",
    "data['D Sell Diff -1'] = 100*(data['Last_Month_D'] - data2['Last_Month_D'])/data2['Last_Month_D']\n",
    "data['E Sell Diff -1'] = 100*(data['Last_Month_E'] - data2['Last_Month_E'])/data2['Last_Month_E']\n",
    "data['F Sell Diff -1'] = 100*(data['Last_Month_F'] - data2['Last_Month_F'])/data2['Last_Month_F']\n",
    "\n",
    "\n",
    "#for -2 month\n",
    "last2usd = last2usd.shift(1)\n",
    "data2 = data.copy().shift(1)\n",
    "\n",
    "usd['USD Diff -2'] = (100*(usd['USD Now'] - last2usd['USD Now'])/last2usd['USD Now']).fillna(usd['USD Diff -1'])\n",
    "usd['EUR Diff -2'] = (100*(usd['EUR Now'] - last2usd['EUR Now'])/last2usd['EUR Now']).fillna(usd['EUR Diff -1'])\n",
    "usd['BIST Diff -2'] = (100*(usd['BIST Now'] - last2usd['BIST Now'])/last2usd['BIST Now']).fillna(usd['BIST Diff -1'])\n",
    "usd['Export Diff -2'] = (100*(usd['Exports'] - last2usd['Exports'])/last2usd['Exports']).fillna(usd['Export Diff -1'])\n",
    "usd['Import Diff -2'] = (100*(usd['Imports'] - last2usd['Imports'])/last2usd['Imports']).fillna(usd['Import Diff -1'])\n",
    "\n",
    "\n",
    "data['A Sell Diff -2'] = (100*(data['Last_Month_A'] - data2['Last_Month_A'])/data2['Last_Month_A']).fillna(data['A Sell Diff -1'])\n",
    "data['B Sell Diff -2'] = (100*(data['Last_Month_B'] - data2['Last_Month_B'])/data2['Last_Month_B']).fillna(data['B Sell Diff -1'])\n",
    "data['C Sell Diff -2'] = (100*(data['Last_Month_C'] - data2['Last_Month_C'])/data2['Last_Month_C']).fillna(data['C Sell Diff -1'])\n",
    "data['D Sell Diff -2'] = (100*(data['Last_Month_D'] - data2['Last_Month_D'])/data2['Last_Month_D']).fillna(data['D Sell Diff -1'])\n",
    "data['E Sell Diff -2'] = (100*(data['Last_Month_E'] - data2['Last_Month_E'])/data2['Last_Month_E']).fillna(data['E Sell Diff -1'])\n",
    "data['F Sell Diff -2'] = (100*(data['Last_Month_F'] - data2['Last_Month_F'])/data2['Last_Month_F']).fillna(data['F Sell Diff -1'])\n",
    "\n",
    "\n",
    "#for -3 month\n",
    "last2usd = last2usd.shift(1)\n",
    "data2 = data.copy().shift(1)\n",
    "\n",
    "usd['USD Diff -3'] = (100*(usd['USD Now'] - last2usd['USD Now'])/last2usd['USD Now']).fillna(usd['USD Diff -2'])\n",
    "usd['EUR Diff -3'] = (100*(usd['EUR Now'] - last2usd['EUR Now'])/last2usd['EUR Now']).fillna(usd['EUR Diff -2'])\n",
    "usd['BIST Diff -3'] = (100*(usd['BIST Now'] - last2usd['BIST Now'])/last2usd['BIST Now']).fillna(usd['BIST Diff -2'])\n",
    "usd['Export Diff -3'] = (100*(usd['Exports'] - last2usd['Exports'])/last2usd['Exports']).fillna(usd['Export Diff -2'])\n",
    "usd['Import Diff -3'] = (100*(usd['Imports'] - last2usd['Imports'])/last2usd['Imports']).fillna(usd['Import Diff -2'])\n",
    "\n",
    "\n",
    "data['A Sell Diff -3'] = (100*(data['Last_Month_A'] - data2['Last_Month_A'])/data2['Last_Month_A']).fillna(data['A Sell Diff -2'])\n",
    "data['B Sell Diff -3'] = (100*(data['Last_Month_B'] - data2['Last_Month_B'])/data2['Last_Month_B']).fillna(data['B Sell Diff -2'])\n",
    "data['C Sell Diff -3'] = (100*(data['Last_Month_C'] - data2['Last_Month_C'])/data2['Last_Month_C']).fillna(data['C Sell Diff -2'])\n",
    "data['D Sell Diff -3'] = (100*(data['Last_Month_D'] - data2['Last_Month_D'])/data2['Last_Month_D']).fillna(data['D Sell Diff -2'])\n",
    "data['E Sell Diff -3'] = (100*(data['Last_Month_E'] - data2['Last_Month_E'])/data2['Last_Month_E']).fillna(data['E Sell Diff -2'])\n",
    "data['F Sell Diff -3'] = (100*(data['Last_Month_F'] - data2['Last_Month_F'])/data2['Last_Month_F']).fillna(data['F Sell Diff -2'])\n",
    "\n",
    "#for -6 month\n",
    "last2usd = last2usd.shift(3)\n",
    "data2 = data.copy().shift(3)\n",
    "\n",
    "usd['USD Diff -6'] = (100*(usd['USD Now'] - last2usd['USD Now'])/last2usd['USD Now']).fillna(usd['USD Diff -3'])\n",
    "usd['EUR Diff -6'] = (100*(usd['EUR Now'] - last2usd['EUR Now'])/last2usd['EUR Now']).fillna(usd['EUR Diff -3'])\n",
    "usd['BIST Diff -6'] = (100*(usd['BIST Now'] - last2usd['BIST Now'])/last2usd['BIST Now']).fillna(usd['BIST Diff -3'])\n",
    "usd['Export Diff -6'] = (100*(usd['Exports'] - last2usd['Exports'])/last2usd['Exports']).fillna(usd['Export Diff -3'])\n",
    "usd['Import Diff -6'] = (100*(usd['Imports'] - last2usd['Imports'])/last2usd['Imports']).fillna(usd['Import Diff -3'])\n",
    "\n",
    "\n",
    "data['A Sell Diff -6'] = (100*(data['Last_Month_A'] - data2['Last_Month_A'])/data2['Last_Month_A']).fillna(data['A Sell Diff -3'])\n",
    "data['B Sell Diff -6'] = (100*(data['Last_Month_B'] - data2['Last_Month_B'])/data2['Last_Month_B']).fillna(data['B Sell Diff -3'])\n",
    "data['C Sell Diff -6'] = (100*(data['Last_Month_C'] - data2['Last_Month_C'])/data2['Last_Month_C']).fillna(data['C Sell Diff -3'])\n",
    "data['D Sell Diff -6'] = (100*(data['Last_Month_D'] - data2['Last_Month_D'])/data2['Last_Month_D']).fillna(data['D Sell Diff -3'])\n",
    "data['E Sell Diff -6'] = (100*(data['Last_Month_E'] - data2['Last_Month_E'])/data2['Last_Month_E']).fillna(data['E Sell Diff -3'])\n",
    "data['F Sell Diff -6'] = (100*(data['Last_Month_F'] - data2['Last_Month_F'])/data2['Last_Month_F']).fillna(data['F Sell Diff -3'])\n",
    "\n",
    "\n",
    "#for -9 month\n",
    "last2usd = last2usd.shift(3)\n",
    "data2 = data.copy().shift(3)\n",
    "\n",
    "usd['USD Diff -9'] = (100*(usd['USD Now'] - last2usd['USD Now'])/last2usd['USD Now']).fillna(usd['USD Diff -6'])\n",
    "usd['EUR Diff -9'] = (100*(usd['EUR Now'] - last2usd['EUR Now'])/last2usd['EUR Now']).fillna(usd['EUR Diff -6'])\n",
    "usd['BIST Diff -9'] = (100*(usd['BIST Now'] - last2usd['BIST Now'])/last2usd['BIST Now']).fillna(usd['BIST Diff -6'])\n",
    "usd['Export Diff -9'] = (100*(usd['Exports'] - last2usd['Exports'])/last2usd['Exports']).fillna(usd['Export Diff -6'])\n",
    "usd['Import Diff -9'] = (100*(usd['Imports'] - last2usd['Imports'])/last2usd['Imports']).fillna(usd['Import Diff -6'])\n",
    "\n",
    "\n",
    "data['A Sell Diff -9'] = (100*(data['Last_Month_A'] - data2['Last_Month_A'])/data2['Last_Month_A']).fillna(data['A Sell Diff -6'])\n",
    "data['B Sell Diff -9'] = (100*(data['Last_Month_B'] - data2['Last_Month_B'])/data2['Last_Month_B']).fillna(data['B Sell Diff -6'])\n",
    "data['C Sell Diff -9'] = (100*(data['Last_Month_C'] - data2['Last_Month_C'])/data2['Last_Month_C']).fillna(data['C Sell Diff -6'])\n",
    "data['D Sell Diff -9'] = (100*(data['Last_Month_D'] - data2['Last_Month_D'])/data2['Last_Month_D']).fillna(data['D Sell Diff -6'])\n",
    "data['E Sell Diff -9'] = (100*(data['Last_Month_E'] - data2['Last_Month_E'])/data2['Last_Month_E']).fillna(data['E Sell Diff -6'])\n",
    "data['F Sell Diff -9'] = (100*(data['Last_Month_F'] - data2['Last_Month_F'])/data2['Last_Month_F']).fillna(data['F Sell Diff -6'])\n",
    "\n",
    "\n",
    "#for -12 month\n",
    "last2usd = last2usd.shift(3)\n",
    "data2 = data.copy().shift(3)\n",
    "\n",
    "usd['USD Diff -12'] = (100*(usd['USD Now'] - last2usd['USD Now'])/last2usd['USD Now']).fillna(usd['USD Diff -9'])\n",
    "usd['EUR Diff -12'] = (100*(usd['EUR Now'] - last2usd['EUR Now'])/last2usd['EUR Now']).fillna(usd['EUR Diff -9'])\n",
    "usd['BIST Diff -12'] = (100*(usd['BIST Now'] - last2usd['BIST Now'])/last2usd['BIST Now']).fillna(usd['BIST Diff -9'])\n",
    "usd['Export Diff -12'] = (100*(usd['Exports'] - last2usd['Exports'])/last2usd['Exports']).fillna(usd['Export Diff -9'])\n",
    "usd['Import Diff -12'] = (100*(usd['Imports'] - last2usd['Imports'])/last2usd['Imports']).fillna(usd['Import Diff -9'])\n",
    "\n",
    "\n",
    "data['A Sell Diff -12'] = (100*(data['Last_Month_A'] - data2['Last_Month_A'])/data2['Last_Month_A']).fillna(data['A Sell Diff -9'])\n",
    "data['B Sell Diff -12'] = (100*(data['Last_Month_B'] - data2['Last_Month_B'])/data2['Last_Month_B']).fillna(data['B Sell Diff -9'])\n",
    "data['C Sell Diff -12'] = (100*(data['Last_Month_C'] - data2['Last_Month_C'])/data2['Last_Month_C']).fillna(data['C Sell Diff -9'])\n",
    "data['D Sell Diff -12'] = (100*(data['Last_Month_D'] - data2['Last_Month_D'])/data2['Last_Month_D']).fillna(data['D Sell Diff -9'])\n",
    "data['E Sell Diff -12'] = (100*(data['Last_Month_E'] - data2['Last_Month_E'])/data2['Last_Month_E']).fillna(data['E Sell Diff -9'])\n",
    "data['F Sell Diff -12'] = (100*(data['Last_Month_F'] - data2['Last_Month_F'])/data2['Last_Month_F']).fillna(data['F Sell Diff -9'])\n",
    "\n",
    "\n",
    "usd = usd.shift(-1)\n",
    "usd = usd.iloc[:,18:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Data with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>Last_Month_A</th>\n",
       "      <th>Last_Month_B</th>\n",
       "      <th>Last_Month_C</th>\n",
       "      <th>Last_Month_D</th>\n",
       "      <th>...</th>\n",
       "      <th>USD Diff -9</th>\n",
       "      <th>EUR Diff -9</th>\n",
       "      <th>BIST Diff -9</th>\n",
       "      <th>Export Diff -9</th>\n",
       "      <th>Import Diff -9</th>\n",
       "      <th>USD Diff -12</th>\n",
       "      <th>EUR Diff -12</th>\n",
       "      <th>BIST Diff -12</th>\n",
       "      <th>Export Diff -12</th>\n",
       "      <th>Import Diff -12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200401</th>\n",
       "      <td>78</td>\n",
       "      <td>11759</td>\n",
       "      <td>9961</td>\n",
       "      <td>3438</td>\n",
       "      <td>257</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.040000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>9.440000</td>\n",
       "      <td>-20.675929</td>\n",
       "      <td>-3.009738</td>\n",
       "      <td>-1.040000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>9.440000</td>\n",
       "      <td>-20.675929</td>\n",
       "      <td>-3.009738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200402</th>\n",
       "      <td>41</td>\n",
       "      <td>11218</td>\n",
       "      <td>9594</td>\n",
       "      <td>2440</td>\n",
       "      <td>287</td>\n",
       "      <td>126</td>\n",
       "      <td>78.0</td>\n",
       "      <td>11759.0</td>\n",
       "      <td>9961.0</td>\n",
       "      <td>3438.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.050708</td>\n",
       "      <td>-3.289080</td>\n",
       "      <td>16.985558</td>\n",
       "      <td>12.952928</td>\n",
       "      <td>33.522041</td>\n",
       "      <td>-2.050708</td>\n",
       "      <td>-3.289080</td>\n",
       "      <td>16.985558</td>\n",
       "      <td>12.952928</td>\n",
       "      <td>33.522041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200403</th>\n",
       "      <td>83</td>\n",
       "      <td>21061</td>\n",
       "      <td>18885</td>\n",
       "      <td>5216</td>\n",
       "      <td>636</td>\n",
       "      <td>201</td>\n",
       "      <td>41.0</td>\n",
       "      <td>11218.0</td>\n",
       "      <td>9594.0</td>\n",
       "      <td>2440.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.935869</td>\n",
       "      <td>1.794044</td>\n",
       "      <td>4.423367</td>\n",
       "      <td>9.801632</td>\n",
       "      <td>25.309450</td>\n",
       "      <td>5.935869</td>\n",
       "      <td>1.794044</td>\n",
       "      <td>4.423367</td>\n",
       "      <td>9.801632</td>\n",
       "      <td>25.309450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200404</th>\n",
       "      <td>206</td>\n",
       "      <td>22033</td>\n",
       "      <td>19682</td>\n",
       "      <td>5113</td>\n",
       "      <td>714</td>\n",
       "      <td>229</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21061.0</td>\n",
       "      <td>18885.0</td>\n",
       "      <td>5216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.245667</td>\n",
       "      <td>9.501689</td>\n",
       "      <td>-9.572242</td>\n",
       "      <td>41.084931</td>\n",
       "      <td>30.151137</td>\n",
       "      <td>12.245667</td>\n",
       "      <td>9.501689</td>\n",
       "      <td>-9.572242</td>\n",
       "      <td>41.084931</td>\n",
       "      <td>30.151137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200405</th>\n",
       "      <td>297</td>\n",
       "      <td>25377</td>\n",
       "      <td>22816</td>\n",
       "      <td>5653</td>\n",
       "      <td>569</td>\n",
       "      <td>202</td>\n",
       "      <td>206.0</td>\n",
       "      <td>22033.0</td>\n",
       "      <td>19682.0</td>\n",
       "      <td>5113.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.094785</td>\n",
       "      <td>11.952758</td>\n",
       "      <td>-11.011088</td>\n",
       "      <td>1.271379</td>\n",
       "      <td>0.187854</td>\n",
       "      <td>13.094785</td>\n",
       "      <td>11.952758</td>\n",
       "      <td>-11.011088</td>\n",
       "      <td>1.271379</td>\n",
       "      <td>0.187854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          A      B      C     D    E    F  Last_Month_A  Last_Month_B  \\\n",
       "200401   78  11759   9961  3438  257  138           NaN           NaN   \n",
       "200402   41  11218   9594  2440  287  126          78.0       11759.0   \n",
       "200403   83  21061  18885  5216  636  201          41.0       11218.0   \n",
       "200404  206  22033  19682  5113  714  229          83.0       21061.0   \n",
       "200405  297  25377  22816  5653  569  202         206.0       22033.0   \n",
       "\n",
       "        Last_Month_C  Last_Month_D       ...         USD Diff -9  EUR Diff -9  \\\n",
       "200401           NaN           NaN       ...           -1.040000    -0.870000   \n",
       "200402        9961.0        3438.0       ...           -2.050708    -3.289080   \n",
       "200403        9594.0        2440.0       ...            5.935869     1.794044   \n",
       "200404       18885.0        5216.0       ...           12.245667     9.501689   \n",
       "200405       19682.0        5113.0       ...           13.094785    11.952758   \n",
       "\n",
       "        BIST Diff -9  Export Diff -9  Import Diff -9  USD Diff -12  \\\n",
       "200401      9.440000      -20.675929       -3.009738     -1.040000   \n",
       "200402     16.985558       12.952928       33.522041     -2.050708   \n",
       "200403      4.423367        9.801632       25.309450      5.935869   \n",
       "200404     -9.572242       41.084931       30.151137     12.245667   \n",
       "200405    -11.011088        1.271379        0.187854     13.094785   \n",
       "\n",
       "        EUR Diff -12  BIST Diff -12  Export Diff -12  Import Diff -12  \n",
       "200401     -0.870000       9.440000       -20.675929        -3.009738  \n",
       "200402     -3.289080      16.985558        12.952928        33.522041  \n",
       "200403      1.794044       4.423367         9.801632        25.309450  \n",
       "200404      9.501689      -9.572242        41.084931        30.151137  \n",
       "200405     11.952758     -11.011088         1.271379         0.187854  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.join(usd)\n",
    "del usd, last2usd, data2\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of the shift NaN values did not included to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[2:150,6:12].join(data.iloc[2:150,18:])\n",
    "X_test = data.iloc[150:-1,6:12].join(data.iloc[150:-1,18:])\n",
    "    \n",
    "y_trainset = data.iloc[2:150,:6]\n",
    "y_testset = data.iloc[150:-1,:6]\n",
    "\n",
    "y_A_train = y_trainset['A']\n",
    "y_A_test = y_testset['A']\n",
    "y_B_train = y_trainset['B']\n",
    "y_B_test = y_testset['B']\n",
    "y_C_train = y_trainset['C']\n",
    "y_C_test = y_testset['C']\n",
    "y_D_train = y_trainset['D']\n",
    "y_D_test = y_testset['D']\n",
    "y_E_train = y_trainset['E']\n",
    "y_E_test = y_testset['E']\n",
    "y_F_train = y_trainset['F']\n",
    "y_F_test = y_testset['F']\n",
    "del y_trainset, y_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(ytrue,ypred):\n",
    "    return np.sqrt(mean_squared_error(ytrue,ypred))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor    \n",
    "rf = RandomForestRegressor(n_estimators=100,oob_score=True)\n",
    "\n",
    "\n",
    "feature_cols = X_train.columns.values\n",
    "features = data[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A, oob score: 0.2851\n",
      "Mean Squared Error on A prediction:  76.6773303148\n",
      "B, oob score: 0.4242\n",
      "Mean Squared Error on B prediction:  5749.06234515\n",
      "C, oob score: 0.6573\n",
      "Mean Squared Error on C prediction:  9906.69803607\n",
      "D, oob score: 0.5965\n",
      "Mean Squared Error on D prediction:  3515.12549314\n",
      "E, oob score: 0.6161\n",
      "Mean Squared Error on E prediction:  625.449569013\n",
      "F, oob score: 0.4011\n",
      "Mean Squared Error on F prediction:  150.968549443\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train,y_A_train)\n",
    "predicted_A = rf.predict(X_test)\n",
    "print(\"A, oob score: %.4F\" % rf.oob_score_)\n",
    "p_A = y_A_test.values\n",
    "error = rmsle(predicted_A,p_A)\n",
    "del p_A\n",
    "print \"Mean Squared Error on A prediction: \",error\n",
    "del error\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_imp_A = pd.DataFrame(feature_cols)\n",
    "feat_imp_A.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train,y_B_train)\n",
    "predicted_B = rf.predict(X_test)\n",
    "print(\"B, oob score: %.4F\" % rf.oob_score_)\n",
    "p_B = y_B_test.values\n",
    "error = rmsle(predicted_B,p_B)\n",
    "del p_B\n",
    "print \"Mean Squared Error on B prediction: \",error\n",
    "del error\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_imp_B = pd.DataFrame(feature_cols)\n",
    "feat_imp_B.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train,y_C_train)\n",
    "predicted_C = rf.predict(X_test)\n",
    "print(\"C, oob score: %.4F\" % rf.oob_score_)\n",
    "p_C = y_C_test.values\n",
    "error = rmsle(predicted_C,p_C)\n",
    "del p_C\n",
    "print \"Mean Squared Error on C prediction: \",error\n",
    "del error\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_imp_C = pd.DataFrame(feature_cols)\n",
    "feat_imp_C.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train,y_D_train)\n",
    "predicted_D = rf.predict(X_test)\n",
    "print(\"D, oob score: %.4F\" % rf.oob_score_)\n",
    "p_D = y_D_test.values\n",
    "error = rmsle(predicted_D,p_D)\n",
    "del p_D\n",
    "print \"Mean Squared Error on D prediction: \",error\n",
    "del error\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_imp_D = pd.DataFrame(feature_cols)\n",
    "feat_imp_D.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train,y_E_train)\n",
    "predicted_E = rf.predict(X_test)\n",
    "print(\"E, oob score: %.4F\" % rf.oob_score_)\n",
    "p_E = y_E_test.values\n",
    "error = rmsle(predicted_E,p_E)\n",
    "del p_E\n",
    "print \"Mean Squared Error on E prediction: \",error\n",
    "del error\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_imp_E = pd.DataFrame(feature_cols)\n",
    "feat_imp_E.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train,y_F_train)\n",
    "predicted_F = rf.predict(X_test)\n",
    "print(\"F, oob score: %.4F\" % rf.oob_score_)\n",
    "p_F = y_F_test.values\n",
    "error = rmsle(predicted_F,p_F)\n",
    "del p_F\n",
    "print \"Mean Squared Error on F prediction: \",error\n",
    "del error\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_imp_F = pd.DataFrame(feature_cols)\n",
    "feat_imp_F.sort_index(ascending=False,inplace=True)\n",
    "del feature_cols, features, importances, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classified = data.iloc[2:-1,12:].copy()\n",
    "classified['A Sell Diff'] = pd.qcut(classified['A Sell Diff'],2,labels=[0,1]).astype(int)\n",
    "classified['B Sell Diff'] = pd.qcut(classified['B Sell Diff'],2,labels=[0,1]).astype(int)\n",
    "classified['C Sell Diff'] = pd.qcut(classified['C Sell Diff'],2,labels=[0,1]).astype(int)\n",
    "classified['D Sell Diff'] = pd.qcut(classified['D Sell Diff'],2,labels=[0,1]).astype(int)\n",
    "classified['E Sell Diff'] = pd.qcut(classified['E Sell Diff'],2,labels=[0,1]).astype(int)\n",
    "classified['F Sell Diff'] = pd.qcut(classified['F Sell Diff'],2,labels=[0,1]).astype(int)\n",
    "\n",
    "classified['A Sell Diff -1'] = pd.qcut(classified['A Sell Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['A Sell Diff -2'] = pd.qcut(classified['A Sell Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['A Sell Diff -3'] = pd.qcut(classified['A Sell Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['A Sell Diff -6'] = pd.qcut(classified['A Sell Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['A Sell Diff -9'] = pd.qcut(classified['A Sell Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['A Sell Diff -12'] = pd.qcut(classified['A Sell Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['B Sell Diff -1'] = pd.qcut(classified['B Sell Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['B Sell Diff -2'] = pd.qcut(classified['B Sell Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['B Sell Diff -3'] = pd.qcut(classified['B Sell Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['B Sell Diff -6'] = pd.qcut(classified['B Sell Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['B Sell Diff -9'] = pd.qcut(classified['B Sell Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['B Sell Diff -12'] = pd.qcut(classified['B Sell Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['C Sell Diff -1'] = pd.qcut(classified['C Sell Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['C Sell Diff -2'] = pd.qcut(classified['C Sell Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['C Sell Diff -3'] = pd.qcut(classified['C Sell Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['C Sell Diff -6'] = pd.qcut(classified['C Sell Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['C Sell Diff -9'] = pd.qcut(classified['C Sell Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['C Sell Diff -12'] = pd.qcut(classified['C Sell Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['D Sell Diff -1'] = pd.qcut(classified['D Sell Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['D Sell Diff -2'] = pd.qcut(classified['D Sell Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['D Sell Diff -3'] = pd.qcut(classified['D Sell Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['D Sell Diff -6'] = pd.qcut(classified['D Sell Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['D Sell Diff -9'] = pd.qcut(classified['D Sell Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['D Sell Diff -12'] = pd.qcut(classified['D Sell Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['E Sell Diff -1'] = pd.qcut(classified['E Sell Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['E Sell Diff -2'] = pd.qcut(classified['E Sell Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['E Sell Diff -3'] = pd.qcut(classified['E Sell Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['E Sell Diff -6'] = pd.qcut(classified['E Sell Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['E Sell Diff -9'] = pd.qcut(classified['E Sell Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['E Sell Diff -12'] = pd.qcut(classified['E Sell Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['F Sell Diff -1'] = pd.qcut(classified['F Sell Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['F Sell Diff -2'] = pd.qcut(classified['F Sell Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['F Sell Diff -3'] = pd.qcut(classified['F Sell Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['F Sell Diff -6'] = pd.qcut(classified['F Sell Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['F Sell Diff -9'] = pd.qcut(classified['F Sell Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['F Sell Diff -12'] = pd.qcut(classified['F Sell Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['USD Diff -1'] = pd.qcut(classified['USD Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['USD Diff -2'] = pd.qcut(classified['USD Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['USD Diff -3'] = pd.qcut(classified['USD Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['USD Diff -6'] = pd.qcut(classified['USD Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['USD Diff -9'] = pd.qcut(classified['USD Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['USD Diff -12'] = pd.qcut(classified['USD Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['EUR Diff -1'] = pd.qcut(classified['EUR Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['EUR Diff -2'] = pd.qcut(classified['EUR Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['EUR Diff -3'] = pd.qcut(classified['EUR Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['EUR Diff -6'] = pd.qcut(classified['EUR Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['EUR Diff -9'] = pd.qcut(classified['EUR Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['EUR Diff -12'] = pd.qcut(classified['EUR Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['BIST Diff -1'] = pd.qcut(classified['BIST Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['BIST Diff -2'] = pd.qcut(classified['BIST Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['BIST Diff -3'] = pd.qcut(classified['BIST Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['BIST Diff -6'] = pd.qcut(classified['BIST Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['BIST Diff -9'] = pd.qcut(classified['BIST Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['BIST Diff -12'] = pd.qcut(classified['BIST Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['Export Diff -1'] = pd.qcut(classified['Export Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Export Diff -2'] = pd.qcut(classified['Export Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Export Diff -3'] = pd.qcut(classified['Export Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Export Diff -6'] = pd.qcut(classified['Export Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Export Diff -9'] = pd.qcut(classified['Export Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Export Diff -12'] = pd.qcut(classified['Export Diff -12'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "\n",
    "classified['Import Diff -1'] = pd.qcut(classified['Import Diff -1'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Import Diff -2'] = pd.qcut(classified['Import Diff -2'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Import Diff -3'] = pd.qcut(classified['Import Diff -3'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Import Diff -6'] = pd.qcut(classified['Import Diff -6'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Import Diff -9'] = pd.qcut(classified['Import Diff -9'],5,labels=[1,2,3,4,5]).astype(int)\n",
    "classified['Import Diff -12'] = pd.qcut(classified['Import Diff -12'],5,labels=[1,2,3,4,5]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, the objective is predicting the increment or decrement since targerts classified binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = classified.iloc[:150,6:]\n",
    "X_test = classified.iloc[150:,6:]\n",
    "\n",
    "y_trainset = classified.iloc[:150,:6]\n",
    "y_testset = classified.iloc[150:,:6]\n",
    "\n",
    "y_A_train = y_trainset['A Sell Diff']\n",
    "y_A_test = y_testset['A Sell Diff']\n",
    "y_B_train = y_trainset['B Sell Diff']\n",
    "y_B_test = y_testset['B Sell Diff']\n",
    "y_C_train = y_trainset['C Sell Diff']\n",
    "y_C_test = y_testset['C Sell Diff']\n",
    "y_D_train = y_trainset['D Sell Diff']\n",
    "y_D_test = y_testset['D Sell Diff']\n",
    "y_E_train = y_trainset['E Sell Diff']\n",
    "y_E_test = y_testset['E Sell Diff']\n",
    "y_F_train = y_trainset['F Sell Diff']\n",
    "y_F_test = y_testset['F Sell Diff']\n",
    "del y_trainset, y_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier For A\n",
      "oob Score: 0.5267\n",
      "Original A values:  [0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0]\n",
      "Predicted A values: [1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0]\n",
      "Error: 0.470588235294\n",
      "Random Forest Classifier For B\n",
      "oob Score: 0.6733\n",
      "Original B values:   [0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0]\n",
      "Predicted B values:  [0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0]\n",
      "Error: 0.294117647059\n",
      "Random Forest Classifier For C\n",
      "oob Score: 0.6867\n",
      "Original C values:   [0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0]\n",
      "Predicted C values:  [0 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0]\n",
      "Error: 0.294117647059\n",
      "Random Forest Classifier For D\n",
      "oob Score: 0.6733\n",
      "Original D values:   [0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0]\n",
      "Predicted D values:  [0 1 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0]\n",
      "Error: 0.294117647059\n",
      "Random Forest Classifier For E\n",
      "oob Score: 0.7067\n",
      "Original E values:   [0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0]\n",
      "Predicted E values:  [0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0]\n",
      "Error: 0.352941176471\n",
      "Random Forest Classifier For F\n",
      "oob Score: 0.6267\n",
      "Original F values:   [0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
      "Predicted F values:  [0 1 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0]\n",
      "Error: 0.352941176471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', \n",
    "                             n_estimators=1000,\n",
    "                             min_samples_split=10,\n",
    "                             min_samples_leaf=1,\n",
    "                             max_features='auto',\n",
    "                             oob_score=True,\n",
    "                             random_state=1,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train, y_A_train)\n",
    "print \"Random Forest Classifier For A\"\n",
    "print(\"oob Score: \"+\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "predicted_A = rf.predict(X_test)\n",
    "\n",
    "print (\"Original A values:  \" + str(y_A_test.values))\n",
    "print (\"Predicted A values: \" + str(predicted_A))\n",
    "print (\"Error: \" + str(np.mean(y_A_test.values != predicted_A)))\n",
    "\n",
    "\n",
    "feature_cols = X_train.columns.values\n",
    "features = data[feature_cols]\n",
    "\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_rfc_A = pd.DataFrame(feature_cols)\n",
    "feat_rfc_A.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_B_train)\n",
    "print \"Random Forest Classifier For B\"\n",
    "print(\"oob Score: \"+\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "predicted_B = rf.predict(X_test)\n",
    "\n",
    "print \"Original B values:   \" + str(y_B_test.values)\n",
    "print \"Predicted B values:  \" + str(predicted_B)\n",
    "print (\"Error: \" + str(np.mean(y_B_test.values != predicted_B)))\n",
    "\n",
    "\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_rfc_B = pd.DataFrame(feature_cols)\n",
    "feat_rfc_B.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_C_train)\n",
    "print \"Random Forest Classifier For C\"\n",
    "print(\"oob Score: \"+\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "predicted_C = rf.predict(X_test)\n",
    "\n",
    "print \"Original C values:   \" + str(y_C_test.values)\n",
    "print \"Predicted C values:  \" + str(predicted_C)\n",
    "print (\"Error: \" + str(np.mean(y_C_test.values != predicted_C)))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_rfc_C = pd.DataFrame(feature_cols)\n",
    "feat_rfc_C.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_D_train)\n",
    "print \"Random Forest Classifier For D\"\n",
    "print(\"oob Score: \"+\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "predicted_D = rf.predict(X_test)\n",
    "\n",
    "print \"Original D values:   \" + str(y_D_test.values)\n",
    "print \"Predicted D values:  \"  + str(predicted_D)\n",
    "print (\"Error: \" + str(np.mean(y_D_test.values != predicted_D)))\n",
    "\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_rfc_D = pd.DataFrame(feature_cols)\n",
    "feat_rfc_D.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_E_train)\n",
    "print \"Random Forest Classifier For E\"\n",
    "print(\"oob Score: \"+\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "predicted_E = rf.predict(X_test)\n",
    "\n",
    "print \"Original E values:   \" + str(y_E_test.values)\n",
    "print \"Predicted E values:  \"  + str(predicted_E)\n",
    "print (\"Error: \" + str(np.mean(y_E_test.values != predicted_E)))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_rfc_E = pd.DataFrame(feature_cols)\n",
    "feat_rfc_E.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_F_train)\n",
    "print \"Random Forest Classifier For F\"\n",
    "print(\"oob Score: \"+\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "predicted_F = rf.predict(X_test)\n",
    "\n",
    "print \"Original F values:   \" + str(y_F_test.values)\n",
    "print \"Predicted F values:  \"  + str(predicted_F)\n",
    "print (\"Error: \" + str(np.mean(y_F_test.values != predicted_F)))\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "feature_cols = feature_cols[indices]\n",
    "feat_rfc_F = pd.DataFrame(feature_cols)\n",
    "feat_rfc_F.sort_index(ascending=False,inplace=True)\n",
    "\n",
    "del feature_cols, features, importances, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data trained on Neural Network with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential # intitialize the ANN\n",
    "from keras.layers import Dense    # create layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 66))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_A_train, batch_size = 32, epochs = 4000,verbose=0)\n",
    "\n",
    "y_pred_A = model.predict(X_test)\n",
    "y_pred_A = np.round(y_pred_A).astype(int)\n",
    "\n",
    "y_pred_A = y_pred_A.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Trained for A\n",
      "Values of A:             [0 1 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0]\n",
      "Predicted values of A:   [1 1 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0]\n",
      "Error : 0.352941176471\n"
     ]
    }
   ],
   "source": [
    "print \"Neural Network Trained for A\"\n",
    "print(\"Values of A:             \" + str(y_A_test.values))\n",
    "print(\"Predicted values of A:   \" + str(y_pred_A))\n",
    "print(\"Error : \" + str(np.mean(y_A_test.values != y_pred_A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 66))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,y_B_train, batch_size=32, epochs = 4000,verbose=0)\n",
    "\n",
    "y_pred_B = model.predict(X_test)\n",
    "y_pred_B = np.round(y_pred_B).astype(int)\n",
    "\n",
    "y_pred_B = y_pred_B.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Trained for B\n",
      "Values of B:             [0 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0]\n",
      "Predicted values of B:   [0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1]\n",
      "Error : 0.294117647059\n"
     ]
    }
   ],
   "source": [
    "print \"Neural Network Trained for B\"\n",
    "print(\"Values of B:             \" + str(y_B_test.values))\n",
    "print(\"Predicted values of B:   \" + str(y_pred_B))\n",
    "print(\"Error : \" + str(np.mean(y_B_test.values != y_pred_B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 66))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,y_C_train, batch_size=32, epochs = 4000,verbose=0)\n",
    "\n",
    "y_pred_C = model.predict(X_test)\n",
    "y_pred_C = np.round(y_pred_C).astype(int)\n",
    "\n",
    "y_pred_C = y_pred_C.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Trained for C\n",
      "Values of C:             [0 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 0]\n",
      "Predicted values of C:   [0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1]\n",
      "Error : 0.352941176471\n"
     ]
    }
   ],
   "source": [
    "print \"Neural Network Trained for C\"\n",
    "print(\"Values of C:             \" + str(y_C_test.values))\n",
    "print(\"Predicted values of C:   \" + str(y_pred_C))\n",
    "print(\"Error : \" + str(np.mean(y_C_test.values != y_pred_C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 66))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,y_D_train, batch_size=32, epochs = 4000,verbose=0)\n",
    "\n",
    "y_pred_D = model.predict(X_test)\n",
    "y_pred_D = np.round(y_pred_D).astype(int)\n",
    "\n",
    "y_pred_D = y_pred_D.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Trained for D\n",
      "Values of D:             [0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0]\n",
      "Predicted values of D:   [0 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 0]\n",
      "Error : 0.235294117647\n"
     ]
    }
   ],
   "source": [
    "print \"Neural Network Trained for D\"\n",
    "print(\"Values of D:             \" + str(y_D_test.values))\n",
    "print(\"Predicted values of D:   \" + str(y_pred_D))\n",
    "print(\"Error : \" + str(np.mean(y_D_test.values != y_pred_D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 66))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,y_E_train, batch_size=32, epochs = 4000,verbose=0)\n",
    "\n",
    "y_pred_E = model.predict(X_test)\n",
    "y_pred_E = np.round(y_pred_E).astype(int)\n",
    "\n",
    "y_pred_E = y_pred_E.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Trained for E\n",
      "Values of E:             [0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 0]\n",
      "Predicted values of E:   [0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0]\n",
      "Error : 0.294117647059\n"
     ]
    }
   ],
   "source": [
    "print \"Neural Network Trained for E\"\n",
    "print(\"Values of E:             \" + str(y_E_test.values))\n",
    "print(\"Predicted values of E:   \" + str(y_pred_E))\n",
    "print(\"Error : \" + str(np.mean(y_E_test.values != y_pred_E)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 66))\n",
    "model.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train,y_F_train, batch_size=32, epochs = 4000,verbose=0)\n",
    "\n",
    "y_pred_F = model.predict(X_test)\n",
    "y_pred_F = np.round(y_pred_F).astype(int)\n",
    "\n",
    "y_pred_F = y_pred_F.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Trained for F\n",
      "Values of F:             [0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0]\n",
      "Predicted values of F:   [0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 1]\n",
      "Error : 0.235294117647\n"
     ]
    }
   ],
   "source": [
    "print \"Neural Network Trained for F\"\n",
    "print(\"Values of F:             \" + str(y_F_test.values))\n",
    "print(\"Predicted values of F:   \" + str(y_pred_F))\n",
    "print(\"Error : \" + str(np.mean(y_F_test.values != y_pred_F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
